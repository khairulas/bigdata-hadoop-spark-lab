version: "3"

services:
  # --- COMPUTE LAYER (Python 3.10) ---
  spark-master:
    image: jupyter/pyspark-notebook:spark-3.3.0
    container_name: spark-master
    user: root
    command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - bigdata-network

  spark-worker:
    image: jupyter/pyspark-notebook:spark-3.3.0
    container_name: spark-worker
    user: root
    depends_on:
      - spark-master
    command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    networks:
      - bigdata-network

  jupyter:
    image: jupyter/pyspark-notebook:spark-3.3.0
    container_name: bigdata-jupyter
    ports:
      - "8888:8888"
      # Expose the driver ports for the worker to connect back
      - "20002:20002" 
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
    user: root
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data 
    networks:
      - bigdata-network
    command: start-notebook.sh --IdentityProvider.token=''

  # --- STORAGE LAYER (HDFS) ---
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870" # HDFS Browser UI
      - "9000:9000" # File System Port
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - bigdata-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    depends_on:
      - namenode
    networks:
      - bigdata-network

networks:
  bigdata-network:
    name: bigdata-network
    driver: bridge