{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5506ebe-cef3-42df-bb66-87213a799c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Connected!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# 1. Setup Environment\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"root\"\n",
    "\n",
    "# 2. Initialize Spark with Docker Network Fixes\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Student_Lab\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"bigdata-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"20002\") \\\n",
    "    .config(\"spark.blockManager.port\", \"20003\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Connected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c0096-610d-4a75-ae61-7dce1e22c385",
   "metadata": {},
   "source": [
    "Step 1: Ingest (Simulate Data Upload)\n",
    "In a real scenario, you would upload a file via the Jupyter interface. For this lab, we will generate a realistic CSV file directly in the notebook to ensure consistent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba2e0da-e6ae-4666-8bbc-839c89b3bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Ingested: Created sales_data_raw.csv with 1000 rows.\n",
      "   transaction_id store_location product_category  quantity  unit_price\n",
      "0               1       New York             Home         5  201.140244\n",
      "1               2          Paris             Home         3  358.504293\n",
      "2               3         London         Clothing         9  420.131376\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Define the dataset\n",
    "data = {\n",
    "    'transaction_id': range(1, 1001),\n",
    "    'store_location': np.random.choice(['New York', 'London', 'Tokyo', 'Paris'], 1000),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], 1000),\n",
    "    'quantity': np.random.randint(1, 10, 1000),\n",
    "    'unit_price': np.random.uniform(10.0, 500.0, 1000)\n",
    "}\n",
    "\n",
    "# 2. Create Pandas DataFrame\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "# 3. Save as CSV to the local container disk (simulating an uploaded file)\n",
    "csv_path = \"sales_data_raw.csv\"\n",
    "pdf.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Data Ingested: Created {csv_path} with {len(pdf)} rows.\")\n",
    "print(pdf.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c069b3-dae1-4c21-8c0e-1f40f391064e",
   "metadata": {},
   "source": [
    "Step 2: Process (Pandas to Spark)\n",
    "Now we act as a Data Engineer. We initialize the Spark engine and convert our raw \"Small Data\" (Pandas) into distributed \"Big Data\" (Spark DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae60124-e549-4ba7-9598-203d17d9942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark Engine...\n",
      "✅ Spark Connected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema inferred by Spark:\n",
      "root\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- store_location: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# 1. Setup Environment (Critical for Docker)\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"root\"\n",
    "\n",
    "print(\"Initializing Spark Engine...\")\n",
    "\n",
    "# 2. Configure Spark Session\n",
    "# THESE LINES MUST USE THE SAME NUMBER OF SPACES:\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab1_Data_Processing\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"bigdata-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"20002\") \\\n",
    "    .config(\"spark.blockManager.port\", \"20003\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark Connected!\")\n",
    "\n",
    "# 3. Convert Pandas DF to Spark DF\n",
    "# This distributes the data across the cluster nodes\n",
    "df_spark = spark.createDataFrame(pdf)\n",
    "\n",
    "print(\"Schema inferred by Spark:\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cec4f-b2cd-46b1-bc9d-afd061c2ef63",
   "metadata": {},
   "source": [
    "Step 3: Store (Write to Data Lake/HDFS)\n",
    "We will now save this processed data into our Data Lake (HDFS). We use Parquet format because it is column-oriented, compressed, and much faster for analytics than CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f458ea25-018e-4a88-8f85-25462226b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to Data Lake: hdfs://namenode:9000/user/data/sales_parquet ...\n",
      "✅ Storage Complete! Data is now safe in HDFS.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define HDFS Path\n",
    "# We write to the namenode container's storage\n",
    "hdfs_path = \"hdfs://namenode:9000/user/data/sales_parquet\"\n",
    "\n",
    "print(f\"Writing data to Data Lake: {hdfs_path} ...\")\n",
    "\n",
    "# 2. Write to HDFS\n",
    "# mode(\"overwrite\") replaces the file if you run this cell again\n",
    "df_spark.write.mode(\"overwrite\").parquet(hdfs_path)\n",
    "\n",
    "print(\"✅ Storage Complete! Data is now safe in HDFS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ea862-2406-4af7-bb0f-7b32a4b3625a",
   "metadata": {},
   "source": [
    "Step 4: Analyze (Read & SQL)\n",
    "Now we act as a Data Analyst. We will read the optimized Parquet file from the Data Lake and run SQL queries to derive insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd106ccf-f299-46f7-a44a-54290422d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from Data Lake...\n",
      "--- SQL ANALYSIS: Total Revenue by City ---\n",
      "+--------------+-------------+------------------+\n",
      "|store_location|total_revenue|avg_items_per_sale|\n",
      "+--------------+-------------+------------------+\n",
      "|         Paris|     337168.4| 5.483606557377049|\n",
      "|         Tokyo|    332754.79| 4.681818181818182|\n",
      "|      New York|     313012.5|  5.08298755186722|\n",
      "|        London|    306234.04| 4.948207171314741|\n",
      "+--------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, avg, round\n",
    "\n",
    "print(\"Reading from Data Lake...\")\n",
    "\n",
    "# 1. Read Parquet from HDFS\n",
    "df_analytics = spark.read.parquet(hdfs_path)\n",
    "\n",
    "# 2. Register as a Temporary SQL View\n",
    "df_analytics.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "print(\"--- SQL ANALYSIS: Total Revenue by City ---\")\n",
    "\n",
    "# 3. Run SQL Query\n",
    "# Calculate Revenue (Quantity * Price) and Group by City\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        store_location, \n",
    "        ROUND(SUM(quantity * unit_price), 2) as total_revenue,\n",
    "        AVG(quantity) as avg_items_per_sale\n",
    "    FROM sales\n",
    "    GROUP BY store_location\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "# 4. Stop the Engine\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92cc9e-7ba3-4859-841f-dfdbeb4ab212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
